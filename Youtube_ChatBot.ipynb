{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsvApmLVD1oqwtdabwwV05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamsinghtomar78/LangChain/blob/main/Youtube_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "6LBIJgdYVAIP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] =\" \""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries**"
      ],
      "metadata": {
        "id": "pTBlCgIM76ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-openai \\\n",
        "               faiss-cpu tiktoken python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBWQpNgv8Ao6",
        "outputId": "6f6b6f01-64b9-4e05-ad4f-4fd28ccc93b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "k93N1DPV8PVK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1a - Indexing (Document Ingestion)**"
      ],
      "metadata": {
        "id": "S-lti4xf8TZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "video_id = \"pZybROKrj2Q\"\n",
        "try:\n",
        "    api = YouTubeTranscriptApi()\n",
        "    transcript_list = api.list(video_id)\n",
        "    transcript_snippet = transcript_list.find_transcript(['en'])\n",
        "    transcript_data = transcript_snippet.fetch()\n",
        "\n",
        "    # Extract text from the FetchedTranscript object\n",
        "    transcript = \" \".join(snippet.text for snippet in transcript_data.snippets)\n",
        "\n",
        "\n",
        "except TranscriptsDisabled:\n",
        "    print(\"No captions available for this video.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "Z06srYWf-wbW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "DvviXm9q_V88",
        "outputId": "27cc6ebf-9eaa-4a56-a400-2acbec856cab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[MUSIC PLAYING] HANNAH FRY: Welcome to \"Google\\nDeepMind, the Podcast\" with me, your host, Professor Hannah Fry. Now, when we first\\nstarted thinking about making this\\npodcast way back in 2017, DeepMind was this relatively\\nsmall, focused AI research lab. They\\'d just been\\nbought by Google and given the freedom to do\\ntheir own quirky research projects from the safe\\ndistance of London. How things have changed. Because since the\\nlast season, Google has reconfigured its\\nentire structure, putting AI and the\\nteam at DeepMind at the core of its strategy. Google DeepMind has\\ncontinued its quest to endow AI with\\nhuman-level intelligence, known as artificial general\\nintelligence, or AGI. It has introduced a family of\\npowerful new AI models called Gemini, as well as\\nan AI agent called Project Astra that can process\\naudio, video, image, and code. The lab is also\\nmaking huge leaps in applying AI to a host\\nof scientific domains, including a brand new\\nthird version of AlphaFold, which can predict the structures\\nof all of the molecules that you will find in the\\nhuman body, not just proteins. And in 2021, they spun off a\\nnew company, Isomorphic Labs, to get down to the\\nbusiness of discovering new drugs to treat diseases. Google DeepMind is also working\\non powerful AI agents that can learn to perform tasks by\\nthemselves using reinforcement learning, and continuing\\nthat legacy of AlphaGo\\'s famous victory over a\\nhuman in the game of Go. Now, of course, you\\'ll all have\\nbeen following this podcast since the beginning. You\\'ll all be familiar\\nwith the stories behind all of those changes. But just in case you are\\ncoming to us fresh, welcome. You can find our first\\naward-winning previous seasons on Google DeepMind\\'s\\nYouTube channel, or wherever you\\nget your podcasts. They also, those\\nepisodes go into detail about a lot of the\\nthemes that we\\'re going to hear come up over\\nand over again from the people here, like reinforcement\\nlearning, deep learning, large language\\nmodels and, so on. So have a listen. They are really good, even\\nif we do say so ourselves. Now, all of the\\nnewfound attention on AI since the last series does\\nmean that there are quite a few more podcasts out there\\nfor you to choose from. But on this podcast, in just\\nthe same way as we always have, we want to offer you something\\na little bit different. We want to take you right to the\\nheart of where these ideas are coming from to introduce\\nyou to the people who are leading the design of\\nour collective future-- no hype, no spin, just\\ncompelling discussions and grand scientific ambition. So with all of that\\nin mind, I am here with the DeepMind\\nco-founder and now CEO of Google DeepMind,\\nDemis Hassabis. So with all of that in mind,\\ndo I have to call you Sir Demis now? DEMIS HASSABIS:\\nNo, absolutely not. HANNAH FRY: OK. Well, Demis, welcome\\nto the podcast. DEMIS HASSABIS: Thank you. HANNAH FRY: Thank you\\nvery much for being here. OK, I want to know, is\\nyour job easier or harder now that there has been this\\nexplosion in public interest? DEMIS HASSABIS: I think\\nit\\'s double edged. I think it\\'s harder\\nbecause there\\'s just so much scrutiny,\\nfocus, and actually quite a lot of noise in\\nthe whole field. I actually preferred it\\nwhen it was less people, and maybe a little bit more\\nfocused on the science. But it\\'s also good because it\\nshows that the technology is ready to impact the real\\nworld in many different ways, and impact people\\'s everyday\\nlives in positive ways. So I think it\\'s exciting, too. HANNAH FRY: Have\\nyou been surprised by how quickly this has caught\\nthe public\\'s imagination? I mean, I guess you would have\\nexpected that eventually people would have got on board. DEMIS HASSABIS: Yes, exactly. So at some point,\\nthose of us who\\'ve been working on it like us for\\nmany years now, even decades, so I guess at some\\npoint the general public would wake up to that fact. And effectively,\\neveryone\\'s starting to realize how important\\nAI is going to be. But it\\'s been\\nquite surreal still to see that actually come\\nto fruition, and for that to happen. And I guess it is the advent\\nof the chat bots and language models because everyone,\\nof course, uses language. Everyone can\\nunderstand language. So it\\'s an easy way\\nfor the general public to understand and maybe\\nmeasure where AI has got to. HANNAH FRY: I heard you\\ndescribe these chat bots as though they were unreasonably\\neffective, which I really like. And actually, later\\nin the podcast we are going to be discussing\\ntransformers, which was the big breakthrough,\\nI guess-- the big advance that gave us those tools. But tell me first, what do you\\nmean by unreasonably effective? DEMIS HASSABIS:\\nWhat I mean by it is I suppose if one were to\\nwind back 5, 10 years ago, and you were to\\nsay the way we\\'re going to go about this is build\\nthese amazing architectures, and then scale from there,\\nand not necessarily crack specific things like\\nconcepts or abstractions. These are a lot of\\ndebates we would have 5, 10 years ago is do you\\nneed a special way of doing abstractions? The brain certainly\\nseems to do that. But yet somehow, the systems,\\nif you give them enough data-- i.e. The whole internet--\\nthen they do seem to learn this\\nand generalize from those examples--\\nnot just rote memorize, but actually somewhat understand\\nwhat they\\'re processing. And it\\'s a little bit\\nunreasonably effective in the sense that I\\ndon\\'t think anyone would have thought that it would\\nwork as well as it has done, say, five years ago. HANNAH FRY: Yeah. I suppose it is a\\nsurprise that things like conceptual\\nunderstanding and abstraction have emerged rather than been-- DEMIS HASSABIS: Yes,\\nand we would have been-- probably we discussed last\\ntime things like concepts and grounding-- grounding language in\\nreal world experience, maybe in simulations or as\\nrobots embodied intelligence, would have been necessary\\nto really understand the world around us. And of course, these\\nsystems are not there yet. They make lots of mistakes. They don\\'t really have a\\nproper model of the world, but they\\'ve got a lot further\\nthan one might expect just by learning from language. HANNAH FRY: I guess we\\nprobably should actually say what grounding is for\\nthose who haven\\'t listened to series 1 and series 2. Because this was a big thing. I mean, we were talking\\nabout this a lot. So do you want to just\\ngive us an overview of what grounding is? DEMIS HASSABIS:\\nGrounding is when-- one of the reasons the\\nsystems that were built in the \\'80s and \\'90s, the\\nclassical AI systems built at places like MIT, they\\nwere big logic systems. So you can imagine them\\nas huge databases of words connected to other words. And the problem was you could\\nsay something, a dog has legs, and that would be\\nin the database. But the problem was, as\\nsoon as you showed it a picture of a dog, it had no\\nidea that collection of pixels was referring to that symbol. And that\\'s the\\ngrounding problem. So you have this\\nsymbolic representation, this abstract representation,\\nbut what does it really mean in the real world-- in the messy real world? And then, of course,\\nthey tried to fix that, but you never get\\nthat quite right. And instead of that, of\\ncourse, today\\'s systems, they\\'re directly\\nlearning from the data. So in a way, they\\'re\\nforming that connection from the beginning. But the interesting thing\\nwas that if you learn just from language, in\\ntheory, there should be missing a lot of the\\ngrounding that you need. But it turns out that a lot\\nof it is inferrable somehow. HANNAH FRY: Why, in theory? DEMIS HASSABIS:\\nWell, because where is that grounding coming from? These systems, at least the\\nfirst large language models-- HANNAH FRY: Don\\'t exist\\nin the real world. DEMIS HASSABIS: --don\\'t\\nexist in the real world. They\\'re not connected\\nto simulators. They\\'re not connected to robots. They don\\'t have any\\naccess to even-- they weren\\'t multimodal\\nto begin with, either. They don\\'t have access to\\nthe visuals or anything else. It\\'s just purely they\\nlive in language space. So they\\'re learning\\nin an abstract domain, so it\\'s pretty surprising they\\ncan then infer some things about the real world from that. HANNAH FRY: Which makes sense if\\nthe grounding gets in by people interacting with the\\nsystem and saying that\\'s a rubbish answer,\\nthat\\'s a good answer. DEMIS HASSABIS: Yes. So for sure, part\\nof that, if the question that they\\'re\\ngetting wrong, the early versions of this,\\nwas due to grounding missing-- actually, the real world\\ndogs bark in this way or whatever it is-- and it\\'s\\nanswering it incorrectly, then that feedback\\nwill correct it. And part of that feedback is\\nfrom our own grounded knowledge. So some grounding is seeping\\nin like that for sure. HANNAH FRY: I remember\\nseeing a really nice example about crossing the English\\nChannel versus walking across the English Channel. DEMIS HASSABIS: Exactly,\\nthose kinds of things. And if it answered wrong,\\nyou would tell it it\\'s wrong. And then it would have\\nto slightly figure out that you can\\'t walk\\nacross the Channel. HANNAH FRY: So some\\nof these properties that have emerged that\\nweren\\'t necessarily expected to be, I want to ask\\nyou a little bit about hype. Do you think that\\nwhere we are right now, how things are at this moment,\\nis overhyped or underhyped? Or is it just hyped, perhaps,\\nin the wrong direction? DEMIS HASSABIS: Yeah, I\\nthink it\\'s more the latter. So I would say that in the\\nnear term, it\\'s hyped too much. So I think people are claiming\\ncan do all sorts of things it can\\'t. There\\'s all sorts of startups\\nand VC money chasing crazy ideas that are just not ready. On the other hand, I think\\nit\\'s still underhyped. HANNAH FRY: Coming\\nfrom you, Demis-- DEMIS HASSABIS: Yes, I\\nknow, I know, I know. HANNAH FRY: AI in 2010. DEMIS HASSABIS:\\nExactly, exactly. But I think it\\'s still\\nunderhyped or perhaps underappreciated\\nstill even now what\\'s going to happen when we\\nget to AGI and post-AGI. I still don\\'t feel\\nlike that\\'s people are quite understood how\\nenormous that\\'s going to be, and therefore, the\\nresponsibility of that. So it\\'s both, really. I think it\\'s a\\nlittle bit overhyped in the near term at the moment. We\\'re going through that cycle. HANNAH FRY: I guess, though,\\nso in terms of all of these potential startups,\\nand VC funding, and so on, you who have\\nlived and breathed this stuff for, as you say, decades, are\\nvery well placed to spot which ones are realistic goals\\nand which ones aren\\'t. But for other people, how\\ncan they distinguish between what\\'s real and what isn\\'t? DEMIS HASSABIS: Yeah, well look,\\nI think you need to look at-- obviously you\\'ve got to do your\\ntechnical due diligence, have some understanding\\nof the technology, and the latest trends. I think also look at, perhaps,\\nthe background of the people saying it, how\\ntechnical they are. Have they just arrived in AI\\nlast year from somewhere else? I don\\'t know. They were doing\\ncrypto last year. These might be some clues\\nthat perhaps they\\'re jumping on a bandwagon. And it doesn\\'t mean to\\nsay, of course, they could still have some good\\nideas, and many will do. But it\\'s a bit more lottery\\nticket like, shall we say. And I think that always happens\\nwhen there\\'s a ton of attention suddenly on a place, and\\nobviously, then the money follows that. And everyone feels like\\nthey\\'re missing out. And that creates a\\nkind of opportunistic, shall we say, environment,\\nwhich is a little bit opposite to those of us\\nwho\\'ve been in for decades in a deep technology, deep\\nscience way, which is ideally the way I think we need to\\ncarry on going as we get closer to AGI. HANNAH FRY: Yeah. And I guess one of the\\nbig things that we\\'re going to talk about\\nin this series is Gemini, which really comes\\nfrom that very deep science approach, I guess. In what ways is Gemini different\\nfrom the other large language models that are\\nreleased by other labs? DEMIS HASSABIS: So from\\nthe beginning with Gemini, we wanted it to be multi-modal\\nfrom the start so it could process not just language, but\\nalso audio, video, image, code-- any modality, really. And the reason we wanted\\nto do that was firstly, we think that\\'s the way to\\nget these systems to actually understand the world around them\\nand build better world models. So actually still going\\nback to our grounding question earlier, still\\nbuilding grounding in, but piggybacking on top\\nof language this time. And so that\\'s important. And we also had this\\nvision in the end of having a universal\\nassistant, and we prototyped something\\ncalled Astro, which I\\'m sure we\\'ll\\ntalk about, which understands not just what you\\'re\\ntyping, but actually the context you\\'re in. And if you think about something\\nlike a personal assistant or digital assistant, it will\\nbe much more useful the more context it understood about\\nwhat you\\'re asking it for or the situation that you\\'re in. So we always thought that would\\nbe a much more useful type of system, and so we\\nbuilt multi-modality in from the start. So that was one thing,\\nnatively multi-modal. And then at the time, that\\nwas the only model doing that. So now the other models\\nare trying to catch up. And then the other\\nbig innovations we had are on memory. So long context. So actually holding in mind 1\\nmillion-- or 2 million now-- tokens, you can think of them\\nas more or less like words, in mind. So you can give it \"War and\\nPeace,\" or even a whole-- because it\\'s multi-modal-- a\\nwhole video now, a whole film, or a lecture, and then get it\\nto answer questions or find you things within that video stream. HANNAH FRY: OK\\nProject Astra, that\\'s the new universal AI\\nagent, the one that can take in video and audio data. At Google. I/O, I think you used\\nthe example of how Astra could help you\\nremember where you left your glasses, for instance. So I wonder, though,\\nabout the lineage of this stuff because is this\\njust a fancy, advanced version of those old Google glasses? DEMIS HASSABIS: So, of course,\\nGoogle have a long history of developing glass-type\\ndevices actually back to, I think, 2012 or something. So they were way\\nahead of the curve. But maybe it was just missing\\nthis kind of technology So you could actually\\nunderstand-- a smart agent, or a smart assistant\\nthat could actually understand what it\\'s seeing. And so we\\'re very excited\\nabout that digital assistant to go around with you and\\nunderstand the world around you. So it seems a really--\\nwhen you use it, it feels a really\\nnatural use case. HANNAH FRY: OK. I want to rewind a tiny\\nbit to the start of Gemini because it came from\\ntwo separate parts of the organization. DEMIS HASSABIS: Yes. So we-- actually, last year\\nwe combined our two research divisions at Alphabet. So obviously, the old DeepMind,\\nand then Google Brain into one we call it super unit, bringing\\nall the talent together-- that amazing talent we\\nhave across the company, across the whole of Google,\\ninto one unified unit. And what it meant was that\\nwe combined all the best knowledge that we had\\nfrom all the research we were doing, but especially\\non language models. So we had Chinchilla, and\\nGopher, and things like that, and they were building\\nthings like PaLM, and LaMDA, and early language models. And they had different\\nstrengths and weaknesses, and we pulled them all together\\ninto what became Gemini as the first Lighthouse project\\nthat the combined group would output. And then the other\\nimportant thing, of course, was bringing together\\nall the compute, as well, so that we could, do\\nthese really massive training runs and actually pull the\\ncompute resources together. So it\\'s been great. HANNAH FRY: I guess,\\nin a lot of ways, the focus of Google Brain and\\nDeepMind was slightly different. Is that fair to say? DEMIS HASSABIS: Yeah. So I think it was. I mean, we were obviously\\nfocused, both of us, on the frontiers\\nof AI, and there was a lot of collaborations\\nalready on a individual research level, but maybe not\\non a strategic level. Obviously, now the combined\\ngroup, Google DeepMind, I describe it as we\\'re the\\nengine room of Google now. But it\\'s worked really well. I think there were a lot\\nmore similarities, actually, in the way we were working\\nthan there were differences, and we\\'ve continued to keep\\nand double down our strengths on things fundamental research. So where does the next\\ntransformer architecture come from? We want to invent that. Obviously, Google Brain\\ninvented the previous one. We combined it with\\ndeep reinforcement learning that we\\npioneered, and I still think more innovations\\nare going to be needed. And I would back us to\\ndo that just as we\\'ve done in the past 10\\nyears collectively, both Brain and DeepMind. So it\\'s been exciting. HANNAH FRY: I want to come\\nback to that merge in a moment. But I think just\\nsticking on Gemini for a second, how good is it? How does it compare\\nto other models? DEMIS HASSABIS: Yeah, well, I\\nthink some of the benchmarks are not-- the problem is\\nthat we need more-- I think this is one thing\\nthe whole field needs is much better benchmarks. HANNAH FRY: Yeah. How do you decide? DEMIS HASSABIS: Well, there\\nare some well-known benchmarks, academic ones. But they\\'re getting\\nsaturated now, and they don\\'t\\nreally differentiate between the nuances, between\\nthe different top models. I would say there\\'s\\nthree models that are at the top, the frontier. So it\\'s Gemini from us,\\nOpenAI\\'s, GPT, of course, and then Anthropic with\\ntheir Claude models. And then obviously, there\\'s\\na bunch of other good models, too, that people like Meta,\\nand Mistral, and others built, and they\\'re differently\\ngood at different things. It depends what you want-- coding, perhaps that\\'s Claude. And reasoning, maybe that\\'s GPT. And then memory\\nstuff, long context, and multimodal understanding,\\nthat would be Gemini. Of course, we\\'re\\ncontinuing to-- all of us are improving our\\nmodels all the time. So given where we started\\nfrom, which Gemini as a project only existed for\\na year, obviously, based on some of\\nour other projects, I think our trajectory\\nis very good. So when we talk next\\ntime, we should hopefully be right at the forefront. HANNAH FRY: Because there\\nis still a way to go. I mean, there are\\nstill some things that these models\\naren\\'t very good at. DEMIS HASSABIS: Yes, for sure. And actually, that\\'s the\\nbig debate right now. So this last set\\nof things emerged from the technologies that were\\ninvented five, six years ago. The question is, they\\'re still\\nmissing a ton of things-- so their factuality, they\\nhallucinate, as we know. They also not good\\nat planning yet. HANNAH FRY: Planning\\nin what sense? DEMIS HASSABIS: Well,\\nlong term planning. So they can\\'t problem solve. Something long term, you\\ngive it an objective, they can\\'t really do actions\\nin the world for you. So they\\'re very much\\nlike passive Q&A systems. You put the energy in\\nby asking the question, and then they give you\\nsome kind of response. But they\\'re not able to\\nsolve a problem for you. You can\\'t say something\\nlike, if you wanted it as a digital assistant, you\\nmight want to say something like, book me that holiday in\\nItaly, and all the restaurants, and the museums, and whatever,\\nand it knows what you like, but then it goes out\\nand books the flights and all of that for you. So it can\\'t do any of that. But I think that\\'s\\nthe next era-- these more agent-based\\nsystems, we would call them,\\nor agentic systems that have agent-like behavior. But of course, that\\'s\\nwhat we\\'re expert in. That\\'s what we used to build\\nwith all our game agents-- AlphaGo and all of\\nthe other things we\\'ve talked in\\nabout in the past. So a lot of what we\\'re\\ndoing is marrying that work that we\\'re,\\nI guess, famous for with the new large\\nmultimodal models. And I think that\\'s going to be\\nthe next generation of systems. You can think of it as\\ncombining AlphaGo with Gemini. HANNAH FRY: Yeah, because I\\nguess AlphaGo was very, very good at planning. DEMIS HASSABIS: Yes, it\\nwas very good at planning. Of course, only in the\\ndomain, though, of games. And so we need to\\ngeneralize that into the general domain of\\neveryday workloads and language. HANNAH FRY: You\\nmentioned a minute ago how Google DeepMind is now\\nthe engine room of Google. I mean, that is\\nquite a big shift since I was last here in the\\nlast couple of years ago. Is Google taking quite\\na big gamble on you? DEMIS HASSABIS:\\nWell, I guess so. I mean, I think Google\\nhave always understood the importance of AI. Sundar, when he\\ntook over as CEO, said that Google was\\nan AI-first company. And we discussed that very\\nearly on in his tenure, and he saw the potential in\\nAI as the next big paradigm shift after mobile and internet,\\nbut bigger than those things. But then I think maybe\\nin the last year or two, we\\'ve really started\\nliving what that means-- not just from a\\nresearch perspective, but also from products\\nand other things. So it\\'s very\\nexciting, but I think it\\'s the right bet for us to\\ncoordinate all of our talents together, and then push\\nas hard as possible. HANNAH FRY: And then how\\nabout the other way around? Because I guess from DeepMind,\\nhaving that very strong research and science focus, does becoming\\nthe engine room for Google now mean that you have to care much\\nmore about commercial interests rather than the\\npurer stuff that-- DEMIS HASSABIS: Yeah,\\nwell, we do definitely have to worry more\\nabout, and it\\'s in our remit now, the\\ncommercial interests. But actually, there\\'s a\\ncouple things say about that. First of all, we\\'re continuing\\non with our science work in AlphaFolds, and you just\\nsaw AlphaFold 3 come out. And we\\'re doubling down\\non our investments there. That\\'s, I think, a unique thing\\nthat we do at Google DeepMind now. And even our competitors\\npoint at those things as universal goods, if you\\nlike, that come out of AI. And that\\'s going really well. And we spun out isomorphic\\nto do drug discovery. So it\\'s very exciting, and\\nthat\\'s all going really well. And so we\\'re going to\\ncontinue to do that. And then was all our work on\\nclimate and all of these things. But then, we\\'re\\nquite a large team, so we can do more than\\none things at once. We\\'re also building our large\\nmodels, Gemini and et cetera, and then we have a product\\nteam that we\\'re building out that is going to bring all\\nthis amazing technology to all of the surfaces that Google has. So it\\'s an incredible\\nprivilege, in a way, to have that there to\\nplug in all of our stuff. And we invent something,\\nit immediately can become useful\\nto a billion people. And so that\\'s really motivating. And actually, the other thing is\\nthere\\'s a lot more convergence now between the technology we\\nneed to develop for a product to have AI in it and what you\\nwould do for pure AGI research purposes. So there\\'s not really--\\nfive years ago, you\\'d have had to build some\\nspecial case AI for a product. Now, you can branch\\noff your main research and, of course, you still\\nneed to do some things that are product specific, but maybe\\nit\\'s only 10% of the work. So there\\'s actually\\nnot that tension anymore between what you would\\ndevelop for an AI product and what you would develop\\nfor trying to build AGI. It\\'s 90%, I would say,\\nthe same research program. And then finally, of\\ncourse, if you do products, and you get them\\nout into the world, you learn a lot from that. And people using\\nit, and you learn a lot about, oh,\\nyour internal metrics don\\'t quite match what\\npeople are saying, so then you can update that. And that\\'s really helpful\\nfor your research. HANNAH FRY: Absolutely. Well, OK, we are going to talk\\na lot more in this podcast about those breakthroughs that\\nhave come from applying AI to science, but I want to\\nask you about that tension that there is between knowing\\nwhen the right moment is to release something\\nto the public. Because internally at\\nDeepMind, those tools like large language models\\nwere being used for research rather than being seen as a\\npotentially commercial thing. DEMIS HASSABIS:\\nYeah, that\\'s right. So as you know, we\\'ve always\\ntaken responsibility incredibly seriously here, and safety,\\nright from the beginning, way back when we started\\nin 2010 and before that. And Google then adopted some\\nof our, basically, ethics charter effectively into\\ntheir AI principles. So we\\'ve always been well\\naligned with the whole of Google and wanting to be responsible\\nabout deploying this as one of the leaders in this space. And so it\\'s been\\ninteresting now starting to ship real products\\nwith Gen AI in them. Actually there\\'s a lot of\\nlearning that is going on, and we\\'re learning\\nfast, which is good because we\\'re at\\nrelatively low stakes here with the\\ncurrent technology. So it\\'s not that powerful yet. But as it gets more powerful,\\nwe have to be more careful. And that\\'s just learning\\nabout the product teams and other groups learning about\\nhow to test Gen AI technologies. It\\'s different from a\\nnormal piece of technology because it doesn\\'t\\nalways do the same thing. It\\'s almost like testing\\nan open world game. It\\'s almost infinite what\\nyou can try and do with it, so it\\'s interesting to\\nfigure out how do you do the red teaming on it. HANNAH FRY: So red\\nteaming, in this case, being where you\\'re competing\\nagainst yourselves? DEMIS HASSABIS: Yeah. So red teaming is when you set\\nup a specific separate team from the team that\\'s\\ndeveloped the technology to stress test it and try and\\nbreak it in any way possible. You actually need to use tools\\nto automate that because nobody can red team-- even if you had\\nthousands of people doing it, that\\'s not enough compared\\nto billions of users when you put it out there. They\\'re going to try\\nall sorts of things. So it\\'s kind of interesting\\nto take that learning, and then improve our processes\\nso that our future launches will be as smooth as possible. And I think we got to do\\nit in stages where there\\'s an experimental phase, then a\\nclosed beta, and then launch-- a little bit,\\nagain, like we used to launch our games\\nback in the day, and learn at each\\nstep of the way. And then the other\\nthing we\\'ve got to do, and I think we need to do more\\non, is use AI itself to help us internally with red\\nteaming and actually spotting some errors\\nautomatically or triaging that so that, then, our\\ndevelopers and human testers can actually focus\\non those hard cases. HANNAH FRY: You said\\nsomething really interesting there about how you\\'re just in\\na much more probabilistic space here. And then, if there\\'s even a\\nvery small chance of something happening, if you\\nhave enough tries, eventually, something\\nwill go wrong. And I guess there have\\nbeen a couple of mistakes that-- public mistakes. DEMIS HASSABIS: Yeah, so\\nthat\\'s why I think that, as I mentioned, that product\\nteams are just getting used to the sorts of testing. They tested these\\nthings, but they have this stochastic nature,\\nprobabilistic nature. So in fact, a lot\\nof cases where if it was a normal piece of\\nsoftware, you could say I\\'ve tested 99.999% of things,\\nso then extrapolates. So then it\\'s enough\\nbecause there\\'s no way of exposing the flaw\\nthat it has if it has one. But that\\'s not the case with\\nthese generative systems. They can do all\\nsorts of things that are a little bit left\\nfield, or out of the box, out of distribution, in a way,\\nfrom what you\\'ve seen before if someone clever or adversarial\\ndecides to-- it\\'s almost like a hacker decides to\\ntest push it in some way. And it could even be-- I mean, it\\'s so\\ncombinatorial, it could even be with all the things\\nthat you\\'ve happened to have said before to it. And then it\\'s in some\\nkind of peculiar state which then-- or it\\'s got\\nits memories filled up with this particular\\nthing, and then that\\'s why it outputs something. So there\\'s a lot of complexity\\nthere, but it\\'s not infinite. So there\\'s ways to deal with it. But it\\'s just a lot more\\nnuanced than launching normal technology. HANNAH FRY: I\\nremember you saying, I think it was in the first\\ntime I interviewed you about how, actually, you have to\\nthink that this is a completely different way of computing. You have to move away from\\nthe things that we completely understand-- the\\ndeterministic stuff-- into this much more messy,\\nprobabilistic error-ridden place, as well as your testers. Do you think the\\npublic slightly has to shift its mindset on the type\\nof computing that we\\'re doing? DEMIS HASSABIS:\\nYeah, I think so, and maybe that\\'s another\\nthing, interestingly, that we\\'re thinking about\\nis actually putting out a kind of principles document\\nor something before you release something to show what is the\\nexpectation from this system. What\\'s it designed for? What\\'s it useful for? What can\\'t it do? And I think there is some sort\\nof education there needed of, you\\'ll be able to find it useful\\nif you do these things with it, but don\\'t try and use it\\nfor these other things because it won\\'t work. And I think that\\nthat\\'s something that we need to get better\\nat clarifying as a field, and then probably users need\\nto get more experienced on. And actually, this interesting. This is probably why chatbots\\nthemselves came a little bit out of the blue. Even obviously ChatGPT, but even\\nto OpenAI, it surprised them. And we had our own chat\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it\\'s like, well, it\\'s\\ngetting these things wrong, and it sometimes hallucinates,\\nand blah, blah, blah. And there\\'s so many things. But then what we didn\\'t\\nrealize is, actually, there\\'s still a lot of very good\\nuse cases for that even now that people find very valuable--\\nsummarizing documents, and really long\\nthings, or writing-- HANNAH FRY: Awkward emails? DEMIS HASSABIS: --awkward\\nemails, or mundane forms to be filled in. And there\\'s all these use\\ncases which, actually, people don\\'t mind if\\nthere\\'s some small errors. They can fix them easily, and\\nsaves a huge amount of time. And I guess that was\\nthe surprising thing. They discovered-- people\\ndiscovered when you put it in the hands of everyone, there\\nwere actually these valuable use cases, even though the systems\\nwere flawed in all of these ways we know. HANNAH FRY: Well, OK, so I\\nthink that sort of takes me on to the next\\nquestion I want to ask, which is about open source. Because when things are\\nin the hands of people, as you mentioned, really\\nextraordinary things can happen. And I know that\\nDeepMind in the past has open sourced lots of\\nits research projects, but it feels like\\nthat\\'s slightly changing now as we go forward. So just tell me what your\\nstance is on open source. DEMIS HASSABIS: Yeah. Well, look, we\\'re\\nhuge supporters of open source and open\\nscience, as you know. I mean, we\\'ve given\\naway and published almost everything we\\'ve done,\\ncollectively, including like, things like transformers,\\nand AlphaGo. We published all these things\\nin \"Nature\" and \"Science.\" AlphaFold was open source,\\nas we covered last time. And these are all good choices,\\nand you\\'re absolutely right. That\\'s the reason\\nthat all works is because that\\'s the way\\ntechnology and science advances as quickly as possible,\\nby sharing information. So almost always,\\nthat\\'s a universal good to do it like that, and\\nthat\\'s how science works. The only exception is when you-- and AGI and powerful\\nAI does fall into this-- is when you have\\na dual purpose technology. And so then, the\\nproblem is that you want to enable all the\\ngood use cases and all the genuine scientists who are\\nacting in good faith and so on, technologists, to build\\non the ideas, critique the ideas, and so on. That\\'s the way society\\nadvances the quickest. But the problem is how\\ndo you restrict access at the same time for bad actors\\nwho would take the same systems, repurpose them for bad\\nends, misuse them-- weapon systems, who knows what? And those general\\npurpose systems can be repurposed like that. And it\\'s OK today\\nbecause I don\\'t think the systems are that powerful. But in two, three,\\nfour years time, especially when you start\\ngetting agent-like systems or agentic behaviors,\\nthen, I think, if something\\'s misused by\\nsomeone, or perhaps even a rogue nation, state,\\nthere could be serious harm. So then, I don\\'t have\\na solution to that. But as a community, we need\\nto think about what does that mean for open source? Perhaps the frontier models need\\nto have more checks on them, and then only after they\\'ve been\\nout for a year or two years, then they can get open sourced. That\\'s the model we\\'re\\nfollowing because we have our own open models\\nof Gemini called Gemma because they\\'re smaller. So they\\'re not frontier models. So their capabilities are very\\nuseful still to the developer because they\\'re also\\neasy to run on a laptop because they\\'re small\\nnumbers of parameters. But the capabilities\\nthey have are well understood at this point. Because they\\'re not\\nfrontier models. So it\\'s just not as\\npowerful as the latest, say, Gemini 1.5 models. So I think that\\'s\\nprobably the approach that we\\'ll end up taking is\\nwe\\'ll have open source models, but they\\'ll be lagging maybe\\none year behind the most cutting edge models just so\\nthat we can really assess out in the open by users what those\\nmodels can do-- the frontier ones can do. HANNAH FRY: And you\\ncan really, I guess, test those boundaries\\nof the stochastic-- DEMIS HASSABIS: Yeah, and\\nwe\\'ll see what those are. The problem with open source\\nis if something goes wrong, you can\\'t recall it. With a proprietary\\nmodel, if your bad actor starts using it in a bad way,\\nyou can just close the tap off. In the limit, you\\ncould switch it off. But once you open\\nsource something, there\\'s no pulling it back. So it\\'s a one way door, so\\nyou should be very, very sure when you do that. HANNAH FRY: Is it\\ndefinitely possible to contain an AGI,\\nthough, within the walls of an organization? DEMIS HASSABIS: Well, that\\'s\\na whole separate question. I don\\'t think we know\\nhow to do that right now. So when you start talking\\nabout AGI level powerful, like human level AI-- HANNAH FRY: Well, what\\nabout intermediary? DEMIS HASSABIS: Well,\\nintermediary, I think, we have good ideas\\nof how to do that. So one would be things\\nlike secure sandboxing. So you test--\\nthat\\'s what I\\'d want to test the agent behaviors\\nin is in a game environment, or a version of\\nthe internet that\\'s not quite fully connected. So there\\'s a lot\\nof security work that\\'s done and known in\\nthis space, and in fintech, and other places. So we\\'d probably\\nborrow those ideas, and then build those\\nkinds of systems. And that\\'s how we would test\\nthe early prototype systems. But we also know\\nthat\\'s not going to be good enough to contain\\nan AGI, something that\\'s potentially smarter than us. So I think we got to understand\\nthose systems better so that we can design the\\nprotocols for an AGI. When that time comes,\\nwe\\'ll have better ideas for how to contain\\nthat, potentially also using AI systems and tools\\nto monitor the next versions of the AI system. HANNAH FRY: So on the\\nsubject of safety, because I know that you are a\\nvery big part of the AI Safety Summit at Bletchley Park in\\n2023, which was, of course, hosted by the UK government. And from the outside,\\nI think a lot of people just say the word regulation\\nas though it\\'s just going to come in and fix everything. But what is your view on how\\nregulation should be structured? DEMIS HASSABIS: Well,\\nI think it\\'s great that governments are getting\\nup to speed on it and involved. I think that\\'s one\\nof the good things about the recent explosion of\\ninterest is that, of course, governments are\\npaying attention. And I think it\\'s been great. The UK government\\nspecifically, who I\\'ve talked to a\\nlot, and US, as well, they\\'ve got very smart\\npeople in the civil service staff that understand the\\ntechnology now to a good degree. And it\\'s been great to see\\nthe AI safety institutes being set up in the\\nUK and US, and I think many other countries\\nare going to follow. So I think these are all\\ngood precedents and protocols to settle into, again, before\\nthe stakes get really high. So this is a proving\\nstage, again, as well. And I do think international\\ncooperation is going to be needed, ideally around\\nthings like regulation, and guardrails, and\\ndeployment norms. So because AI is a digital\\ntechnology, very much so, it\\'s hard to contain it\\nwithin national boundaries. So if the UK or Europe does\\nsomething, or even the US, but China doesn\\'t, does\\nthat really help the world? When we start getting\\ncloser to AGI, not really. So I think my view\\non it is you\\'ve got to be, because the\\ntechnology is changing so fast, we\\'ve got to be very nimble and\\nlight-footed with regulation so that it\\'s easy to adapt it to\\nwhere the latest technology is going. If you\\'d regulated\\nAI five years ago, you\\'d have regulated something\\ncompletely different to what we see today, which is Gen AI. But it might be different\\nagain in five years. It might be these\\nagent-based systems that are the ones that\\ncarry the highest risk. So right now, I would recommend\\nto beef up existing regulations in domains that already have\\nthem-- health, transport, so on. I think you can update\\nthem for an AI world just like they were updated\\nfor mobile and internet. That\\'s probably\\nthe first thing I\\'d do, while doing a watching brief\\non making sure you understand and test the frontier systems. And then as things become\\nclear and more clearly obvious, then start regulating\\naround that. Maybe in a couple of years\\ntime would make sense. One of the things we\\'re missing\\nis, again, the benchmarks-- the right tests for\\ncapabilities that-- what we\\'d all want\\nto know, including the industry in the field, is\\nat what point are capabilities posing some big risk? And there\\'s no answer to that at\\nthe moment beyond what I\\'ve just said, which is agent-based\\ncapabilities is probably our next threshold. But there\\'s no\\nagreed-upon test for that. One thing you might imagine\\nis testing for deception, for example, as a capability. You really don\\'t want\\nthat in the system because then you can\\'t\\nrely on anything else that it\\'s reporting. So that would be my number one\\nemerging capability that I think would be good to test for. But there\\'s many-- ability\\nto achieve certain goals, the ability to replicate. And there\\'s quite a lot of\\nwork going on on this now. And I think the safety\\ninstitutes, which are basically government agencies, I think\\nit would be great for them to push on that, as well. As well as the labs, of course,\\ncontributing what we know. HANNAH FRY: I wonder, in\\nthis picture of the world that you\\'re describing, what\\'s\\nthe place for institutions in this? I mean, if we get\\nto the stage where we have AGI that\\'s supporting\\nall scientific research, is there still a place\\nfor great institutions? DEMIS HASSABIS:\\nYeah, I think so. There\\'s the stage\\nup to AGI, and I think that\\'s got\\nto be a cooperation between civil society,\\nacademia, government, and the industrial labs. So I really believe\\nthat\\'s the only way we\\'re going to get to\\nthe final stages of this. Now, if you\\'re asking\\nafter AGI happens, maybe that is what you\\'re asking,\\nthen AGI, of course, one of the reasons I\\'ve always\\nwanted to build it is then we can use it to start\\nanswering some of the biggest, most fundamental\\nquestions about the nature of reality, and physics,\\nand all of these things, and consciousness, and so on. It depends what form\\nthat takes, whether that will be a human-expert\\ncombination with AI. I think that will be the\\ncase for a while in terms of discovering\\nthe next frontier. So like right now, these\\nsystems can\\'t come up with their own\\nconjectures or hypotheses. They can help you\\nprove something, and I think we\\'ll\\nbe able to prove get gold medals on\\ninternational maths Olympiad, things like that. But maybe even solve\\na famous conjecture. I think that\\'s within reach now. But they don\\'t have\\nthe ability to come up with Riemann hypothesis\\nin the first place, or general relativity. So that, really,\\nwas always my test for maybe a true artificial\\ngeneral intelligence is it will be able to\\ndo that, or invent Go. And so we don\\'t\\nhave any systems. We don\\'t really know how we\\nwould design, in theory, even, a system that could do that. HANNAH FRY: You know\\nthe computer scientist, Stuart Russell? So he told me that he was a bit\\nworried that once we get to AGI, it might be that we all\\nbecome like the royal princes of the past-- the ones who never had to ascend\\nthe throne or do any work, but just got to live this\\nlife of unbridled luxury and have no purpose. DEMIS HASSABIS: Yeah, so that is\\nthe interesting question, is it? Maybe it\\'s beyond AGI. It\\'s more like artificial\\nsuperintelligence or something-- sometimes people call it ASI. But then we should\\nhave radical abundance. And assuming we make sure\\nwe distribute that fairly and equitably, then we\\nwill be in this position where we\\'ll have more\\nfreedom to choose what to do. And then meaning will be a\\nbig philosophical question. And I think we\\'ll\\nneed philosophers, perhaps theologians\\neven, to start thinking as social scientists. They should be thinking\\nabout that now. What brings meaning? I mean, I still think there\\'s,\\nof course, self-actualization, and I don\\'t think we\\'ll all just\\nbe sitting there meditating. But maybe we\\'ll be\\nplaying computer games. I don\\'t know. But is that a bad\\nthing even, or not? Who knows? HANNAH FRY: I don\\'t think\\nthe princes of the past came off particularly well. DEMIS HASSABIS: No. Traveling the stars. But then there\\'s also\\nextreme sports people do. Why do they do them? I mean, climb\\nEverest, all these. I mean, there\\'ll\\nbe-- but I think it\\'s going be very interesting. And that I don\\'t\\nknow, but that\\'s what I was saying\\nearlier about it\\'s underappreciated what\\'s going\\nto happen going back to the hype near-term versus far-term. So if you want to\\ncall that hype, even, it\\'s definitely\\nunder hyped, I think, the amount of\\ntransformation that will happen. I think it will be\\nvery good in the limit. We\\'ll cure lots of diseases,\\nand/or all diseases, solve our energy problems,\\nclimate problems. But then the next question\\ncomes is, is there meaning? HANNAH FRY: So bring us\\nback slightly closer to AGI rather than superintelligence. I know that your big mission is\\nto build artificial intelligence to benefit everybody,\\nbut how do you make sure that it does benefit everybody? How do you include all\\npeople\\'s preferences rather than just the designers? DEMIS HASSABIS:\\nYeah, I think what\\'s going to have to happen is-- I mean, it\\'s impossible\\nto include all preferences in one system. Because by definition,\\npeople don\\'t agree. We can see that\\nin, unfortunately, in the current\\nstate of the world. Countries don\\'t agree. Governments don\\'t agree. We can\\'t even get\\nagreement on obvious things like dealing with the\\nclimate situation. So I think that\\'s very hard. What I imagine will\\nhappen is that we\\'ll have a set of safe\\narchitectures, hopefully, that personalized\\nAIs can be built on top of. And then everyone will\\nhave, or different countries will have their own preferences\\nabout what they use it for, what they deploy it for, what\\ncan and can\\'t be done with them. But overall-- and that\\'s fine. That\\'s for everyone\\nto individually decide or countries to\\ndecide themselves, just like they do today. But as a society,\\nwe know that there\\'s some provably safe things\\nabout those architectures. And then you can let them\\nproliferate, and so on. So I think that we\\'ve got to\\nget through the eye of a needle in a way where as we\\nget closer to AGI, we\\'ve probably got to cooperate\\nmore, ideally internationally, and then make sure we build\\nAGIs in a safe architecture way. Because I\\'m sure\\nthere are unsafe ways, and I\\'m sure there are\\nsafe ways of building AGI. And then once we\\nget through that, then we can open\\nthe funnel again, and everyone can have their\\nown personalized pocket AGIs, if they want. HANNAH FRY: What a\\nversion of the future. But then, in terms of\\nthe safe way to build it, I mean, are we talking\\nabout undesirable behaviors here that might emerge? DEMIS HASSABIS: Yes, undesirable\\nemergent behaviors, capabilities that-- HANNAH FRY: Deception. DEMIS HASSABIS: Deception is\\none example that you don\\'t want. Value systems. We got to understand\\nall of these things better-- what kind of guardrails\\nwork, not circumventable. And there\\'s two\\ncases to worry about. There\\'s bad uses by bad\\nindividuals or nations, so human misuse, and then\\nthere\\'s the AI itself as it gets closer to\\nAGI going off the rails. And I think you need different\\nsolutions for those two problems. And so, yeah, that\\'s\\nwhat we\\'re going to have to contend\\nwith as we get closer to building these technologies. And also, just going back to\\nyour benefiting everyone point, of course, we\\'re showing\\nthe way with things like AlphaFold and isomorphic. I think we could cure most\\ndiseases within the next decade or two if AI drug design works. And then they could be\\npersonalized medicines where it minimizes the side\\neffects on the individual because it\\'s mapped\\nto the person\\'s individual illness, and\\ntheir individual metabolism, and so on. So these are amazing things-- clean energy, renewable\\nenergy sources, fusion, or better solar power,\\nall of these types of things. I think they\\'re\\nall within reach. And then that would\\nsort out water access because you could do\\ndesalination everywhere. So I just feel like\\nan enormous good is going to come from\\nthese technologies, but we have to mitigate\\nthe risks, too. HANNAH FRY: And one\\nway that you said that you would want\\nto mitigate the risks was that there would be a\\nmoment where you would basically do the scientific version\\nof Avengers assemble. DEMIS HASSABIS: Yes, sure. HANNAH FRY: Terence Tao,\\nget him on the phone. DEMIS HASSABIS: Exactly. HANNAH FRY: Bring him on down. DEMIS HASSABIS: Yeah, exactly. HANNAH FRY: Is that\\nstill your plan? DEMIS HASSABIS: Yeah,\\nwell, I think so. I think if we can get the\\ninternational cooperation, I\\'d love there to be a\\ninternational CERN, basically, for AI. Where you get the top\\nresearchers in the world, and you go, look, let\\'s focus on\\nthe final few years of this AGI project, and get\\nit really right, and do it scientifically,\\nand carefully, and thoughtfully at every step-- the final steps. I still think that\\nwould be the best way. HANNAH FRY: How do you know when\\nis the time to press the button? DEMIS HASSABIS: Well,\\nthat\\'s the big question. Because you can\\'t do it\\ntoo early because you would never be able to\\nget the buy-in to do that. A lot of people would disagree. I mean, today people\\ndisagree with the risks. You see very famous people\\nsaying there\\'s no risks, and then you have people\\nlike Jeff Hinton saying there\\'s lots of risks. And I\\'m in the middle of that. HANNAH FRY: I want\\nto talk to you a bit more about neuroscience. How much does it still\\ninspire what you\\'re doing? Because I noticed the other\\nday that DeepMind had unveiled this computerized rat\\nwith an artificial brain that helps to change\\nour understanding of how the brain controls movement. But in the first\\nseason of the podcast, I remember we talked\\na lot about how DeepMind takes\\ndirect inspiration from biological systems. Is that still the\\ncore of your approach? DEMIS HASSABIS: No,\\nit\\'s evolved now because I think we\\'ve\\ngot to a stage now. In the last, I would\\nsay, two to three years, we\\'ve gone more into\\nan engineering phase-- large scale systems, massive\\ntraining architectures. So I would say that the\\ninfluence of neuroscience on that is a little bit less. It may come back in. So any time where you\\nneed more invention, then you want to get as\\nmany sources as possible. And neuroscience would be one\\nof those sources of ideas. But when it\\'s more\\nengineering heavy, then I think that takes a\\nlittle bit more of a backseat. So it may be more applying\\nAI to neuroscience now like you saw with the\\nvirtual rat brain. And I think we\\'ll see that\\nas we get closer to AGI using that to\\nunderstand the brain. I think it would be\\none of the coolest use cases for AGI and science. HANNAH FRY: I guess\\nthis stuff goes through phases of the\\nengineering challenge, the intervention challenge. DEMIS HASSABIS: So it\\'s\\ndone its part for now, and it\\'s been great. And we still obviously\\nkeep a close track of it and take any other ideas, too. HANNAH FRY: OK. All of the pictures of the\\nfuture that you\\'ve painted are still anchored\\nquite in reality. But I know that you\\'ve\\nsaid that you really want AGI to be able to peer into\\nthe mysteries of the universe down at the Planck scale. DEMIS HASSABIS: Yes! HANNAH FRY: Like\\nsubatomic, quantum worlds. Do you think that there are\\nthings that we have not even yet conceived of that might\\nend up being possible? I\\'m talking wormholes here. DEMIS HASSABIS: Completely, yes. I\\'d love wormholes\\nto be possible. I think there is a lot of\\nprobably misunderstanding, I would say, still things we\\ndon\\'t understand about physics and the nature of reality. And obviously, the quantum\\nmechanics, and unifying that with gravity, and\\nall of these things, and there\\'s all these problems\\nwith the standard model. So I think there\\'s-- and string theory, I\\nmean, I just think-- HANNAH FRY: There\\'s giant\\ngaping holes in physics. DEMIS HASSABIS: Yes, in\\nphysics, all over the place. And I talk to my physics\\nfriends about this, and there\\'s a lot of things\\nthat don\\'t fit together. I don\\'t really like the\\nmultiverse explanation. So I think that it would\\nbe great to come up with new theories and then test\\nthose on massive apparatus, perhaps out in space,\\nat these tiny-- the reason I\\'m obsessed\\nwith Planck scale things-- Planck time, Planck space-- is because that seems to be the\\nresolution of reality in a way. That\\'s the smallest quanta\\nyou can break anything into. So that feels like\\nthe level you want to experiment on if you had\\npowerful apparatus perhaps designed or enabled by having\\nAGI and radical abundance. You would need both to be\\nable to afford to build those types of experiments. HANNAH FRY: The\\nresolution of reality. What a phrase. What, so as in the resolution\\nthat we\\'re at the moment? Human level is just an\\napproximation of reality. DEMIS HASSABIS:\\nYes, that\\'s right. And then we know there\\'s\\nthe atomic level, and below that is the Planck\\nlevel, which as far as we know is the smallest resolution one\\ncan even talk about things. And so that, to me,\\nwould be the resolution one wants to experiment\\non to really understand what\\'s going on here. HANNAH FRY: I wonder\\nwhether you\\'re also envisaging that there\\nwill be things that are beyond the limits\\nof human understanding AGI will help us to uncover-- that actually, we\\'re just not\\nreally capable of understanding. And then I wonder if\\nthings are unexplainable or ununderstandable, are\\nthey still falsifiable? DEMIS HASSABIS: Yeah\\nwell, look, I mean, these are great questions. I think there will be a\\npotential for an AGI system to understand higher level\\nabstractions than we can. So again, going back\\nto neuroscience, we know that it\\'s your\\nprefrontal cortex that does that. And there\\'s up to about six or\\nseven layers of indirection one could take. This person\\'s\\nthinking this, and I\\'m thinking this about that person\\nthinking this, and so on. And then we lose track. But I think an AI system could\\nhave an arbitrarily large prefrontal cortex effectively. So you could imagine\\nhigher levels of abstraction and\\npatterns that it will be able to see\\nabout the universe that we can\\'t really comprehend\\nor hold in mind at once. And then I think in terms of\\nexplainability point of view, the way I think\\nthat is a little bit different to other philosophers\\nwho\\'ve thought about this, which is we\\'ll be closer to an ant and\\nthen the AGI, in terms of IQ. But I don\\'t think that\\'s\\nthe way to think of it. I think it\\'s we are\\nTuring complete, so we\\'re a full general\\nintelligence as ourselves, albeit a bit slow because\\nwe run on slow machinery. And we can\\'t infinitely\\nexpand our own brains. But we can, in theory, given\\nenough time and memory, understand anything\\nthat\\'s computable. And so I think it will be more\\nlike Garry Kasparov or Magnus Carlsen playing an\\namazing chess move. I couldn\\'t have come up with it,\\nbut they can explain it to me why it\\'s a good move. So I think that\\'s what an AGI\\nsystem will be able to do. HANNAH FRY: You said that\\nDeepMind was a 20-year project. How far through are we? Are you on track? DEMIS HASSABIS: I think we\\'re\\non track, yeah, crazily. Because usually 20-year\\nprojects stay 20 years away. But yeah, we\\'re a good way\\nin now, and I think we\\'re-- HANNAH FRY: 20 years\\nis 2030 for AGI. DEMIS HASSABIS: 2030, yeah. So I think the way I say\\nis I wouldn\\'t be surprised if it comes in the next decade. So I think we\\'re on track. HANNAH FRY: That matches\\nwhat you said last time. You haven\\'t updated your priors. [LAUGHTER] DEMIS HASSABIS: Exactly HANNAH FRY: Amazing. Demis, thank you so much. Absolute delight. Absolute delight, as always. DEMIS HASSABIS: Very fun to\\ntalk, as always, as well. Thank you. HANNAH FRY: OK, I think there\\nare a few really important things that came out of that\\nconversation, especially when you compare it to what\\nDemis was saying last time we spoke to him in 2022. Because there have definitely\\nbeen a few surprises in the last couple of years. The way that these\\nmodels have demonstrated a genuine conceptual\\nunderstanding is one-- this real world grounding\\nthat came in from language and human feedback alone. We did not think\\nthat would be enough. And then how interesting\\nand useful, imperfect AI has been to the everyday person. Demis himself there\\nadmitted that he had not seen that one coming. And that makes me wonder\\nabout the other challenges that we don\\'t yet\\nknow how to solve like long-term planning, and\\nagency, and robust, unbreakable safeguards. How many of those-- which we\\'re going to cover\\nin detail in this podcast, by the way-- are we going to come back\\nto in a couple of years and realize that they were\\neasier than we thought? And how many of them\\nare going to be harder? And then as for\\nthe big predictions that Demis made, like cures for\\nmost diseases in 10 or 20 years, or AGI by the end of\\nthe decade, or how we\\'re about to enter\\ninto an era of abundance, I mean, they all sound like\\nDemis is being a bit overly optimistic, doesn\\'t it? But then again, he hasn\\'t\\nexactly been wrong so far. You\\'ve been listening to \"Google\\nDeepMind, the Podcast\" with me, Professor Hannah Fry. If you have enjoyed this\\nepisode, hey, why not subscribe? We\\'ve got plenty more\\nfascinating conversations with the people at\\nthe cutting edge of AI coming up on topics ranging\\nfrom how AI is accelerating the pace of\\nscientific discoveries to addressing some\\nof the biggest risks of this technology. If you have any feedback, or you\\nwant to suggest a future guest, then do leave us a\\ncomment on YouTube. Until next time. [MUSIC PLAYING]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1b - Indexing (Text Splitting)**"
      ],
      "metadata": {
        "id": "Cob4EQ56AmOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ],
      "metadata": {
        "id": "tURbnpC-Alxo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SpiX8zzAPU9",
        "outputId": "c3c23331-8fbe-4a8b-f231-d5ffe66e09d0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzy9ywcoA99A",
        "outputId": "83a841a2-6077-4c60-e652-6f60d7ac35fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"the grounding gets in by people interacting with the\\nsystem and saying that's a rubbish answer,\\nthat's a good answer. DEMIS HASSABIS: Yes. So for sure, part\\nof that, if the question that they're\\ngetting wrong, the early versions of this,\\nwas due to grounding missing-- actually, the real world\\ndogs bark in this way or whatever it is-- and it's\\nanswering it incorrectly, then that feedback\\nwill correct it. And part of that feedback is\\nfrom our own grounded knowledge. So some grounding is seeping\\nin like that for sure. HANNAH FRY: I remember\\nseeing a really nice example about crossing the English\\nChannel versus walking across the English Channel. DEMIS HASSABIS: Exactly,\\nthose kinds of things. And if it answered wrong,\\nyou would tell it it's wrong. And then it would have\\nto slightly figure out that you can't walk\\nacross the Channel. HANNAH FRY: So some\\nof these properties that have emerged that\\nweren't necessarily expected to be, I want to ask\")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)**"
      ],
      "metadata": {
        "id": "rBft_Ey9BFgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "yW0CKqf9BBAi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.index_to_docstore_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CLY5XbRBoJc",
        "outputId": "08647471-db53-4fdc-c7a2-c49a759173d7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '43ecd37e-912f-4590-b2eb-8b623fe05a8d',\n",
              " 1: '1206fb8d-a606-4f90-bf8a-e38e6d474e9e',\n",
              " 2: '6a4734bb-c277-4209-82f5-5d17a44e4776',\n",
              " 3: '3c6a6e71-1547-4c82-b94c-09a7d59babea',\n",
              " 4: '55055b13-e7de-467b-ab0c-805c374d553b',\n",
              " 5: 'ceb2973d-98bc-4dcb-b581-b271c0656060',\n",
              " 6: '68d36662-ff62-4836-ab09-45c38520a790',\n",
              " 7: '7d627bd3-6f97-4486-87ae-38649dfc62e6',\n",
              " 8: 'fd1528be-1390-413b-ae82-a2936fd5e1d3',\n",
              " 9: '90d50b97-f4e5-4f08-9fba-e89d41342023',\n",
              " 10: '37b2490f-2608-4b42-a3d6-df6742e2cebb',\n",
              " 11: '9c0b6ca9-5e12-40e2-9e0e-38974a17a770',\n",
              " 12: '37b80dc8-74e8-4c80-82bd-f4a8a3666fd4',\n",
              " 13: '792bde70-dd71-4e03-9560-5cb8619c016c',\n",
              " 14: 'dd069526-b918-4def-8f4a-a9b3a2c72775',\n",
              " 15: '669878a9-b313-46c1-8184-c0fb45617995',\n",
              " 16: 'bf924fab-ff99-4464-a89c-30c82d28a59f',\n",
              " 17: 'c319e9db-d7d2-4123-a9e3-ae34bc8c8e17',\n",
              " 18: '6adcb616-f5e3-4688-a833-3076b17ca9ff',\n",
              " 19: '1aa8b905-2660-47c9-ba05-f7f0bacd086b',\n",
              " 20: 'bf5d7a8e-8e6a-4212-b54b-324ced078245',\n",
              " 21: '4de89ce7-7031-47cd-8085-9d6df54c2c38',\n",
              " 22: 'e4e2acdc-c860-484a-acc6-561857417640',\n",
              " 23: '7413ec88-05cf-4348-8535-533861f5129b',\n",
              " 24: 'ff4e8339-1ef5-42c9-8955-ca1367fc7397',\n",
              " 25: '0eedc3eb-b2d1-4f8b-a7c2-612fb0542cf6',\n",
              " 26: '55515eeb-bf15-408d-84b2-66d4960da25f',\n",
              " 27: '77d4f245-2600-4496-b98e-0675ef260296',\n",
              " 28: '75a6945d-03b7-4007-9e73-41074320cece',\n",
              " 29: '86a8cfeb-e290-4ee1-8a56-9406040306bb',\n",
              " 30: '6cea4536-76c4-4bcd-aea7-cc4170380dee',\n",
              " 31: 'ffbd1c53-75c9-4773-8e44-5562b59d6585',\n",
              " 32: '0209c398-7435-4225-82e1-f5be8abd1d26',\n",
              " 33: 'c92c9acf-b3b5-425d-b3ac-ed22e879645b',\n",
              " 34: '855d2d15-8979-4006-8941-73c567cd9669',\n",
              " 35: 'd8055134-be58-4a0b-ad4b-97860834cbbe',\n",
              " 36: 'e0966fae-658c-4afe-8fee-4e83cb5ca641',\n",
              " 37: 'a429f05d-769d-4505-9fcb-1a8b3362a784',\n",
              " 38: 'fdaa56ea-5c44-4c5d-bc6d-54a4e8a45d90',\n",
              " 39: '96246ca2-1af5-482d-a616-c1c8abd5f38e',\n",
              " 40: 'e8762194-9b7b-46e9-9fa9-20657aa268a2',\n",
              " 41: 'ad51cc2f-0768-419f-937c-bff17297408b',\n",
              " 42: '97baacf8-6a81-4f82-b991-c2e1748f3610',\n",
              " 43: '4e546fa9-e67c-4763-998f-fc17b7335192',\n",
              " 44: '7aaa1873-770f-4744-af03-3905d5b900b3',\n",
              " 45: '7400e563-1f93-4528-8c8f-5b9d4ddb2c50',\n",
              " 46: 'cddbab42-e137-40c9-9b88-1748f24c9a79',\n",
              " 47: '36bcc2a0-454b-470f-aa2f-ee2d84065297',\n",
              " 48: 'f229623b-eacc-4177-b331-2eefc24936e9',\n",
              " 49: 'd8152265-912a-49fa-b42f-f9df8169ed08',\n",
              " 50: '4b56707a-d3bb-4fd2-a5b3-adbd5e9ecfaa',\n",
              " 51: '56deaa10-7581-459a-8283-cdb64d99a2b0',\n",
              " 52: '9af209d5-a03b-4175-b08f-db4bd5b54ff9',\n",
              " 53: 'a4c0ebca-3bc7-4659-b78d-a1dbc592d16c',\n",
              " 54: '70259418-39ae-45c2-b8a4-917ba7891a71',\n",
              " 55: '1c12b0f0-db37-4609-8248-ee7c3d3625d3',\n",
              " 56: '2ea6a6a8-f54a-4d5e-952f-589182433769',\n",
              " 57: 'f2ca74a0-bfcf-4ab1-b949-6c6e776fd771',\n",
              " 58: 'ef497b0e-6452-49fa-ad75-9e5095fc7d84',\n",
              " 59: '7fbb573d-2cdc-4ee7-8893-eeb3691877b7',\n",
              " 60: '7afef696-4f7f-4f7f-a88c-4a353c19eb85',\n",
              " 61: '60f2df77-a004-4d69-9a85-9687fc9fb73c',\n",
              " 62: 'd4bd9d11-b994-4f58-bf5b-4db22764f202',\n",
              " 63: 'd873ebd5-57bf-4e4d-b7ac-460ca54bc293',\n",
              " 64: '3c3203d3-89e8-4cd2-ba87-c3772ef6f5f4',\n",
              " 65: '11827be9-597d-4760-9b2e-b3749aa56106',\n",
              " 66: '0497fe20-18eb-43ed-a698-02520908f8b9',\n",
              " 67: '88486cff-91e2-48f7-811c-0565d1d11554'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.get_by_ids(['88486cff-91e2-48f7-811c-0565d1d11554'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcT69inlCHXV",
        "outputId": "a9adc5a0-823e-4327-c64d-e1fdb1d76af3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='88486cff-91e2-48f7-811c-0565d1d11554', metadata={}, page_content='are going to be harder? And then as for\\nthe big predictions that Demis made, like cures for\\nmost diseases in 10 or 20 years, or AGI by the end of\\nthe decade, or how we\\'re about to enter\\ninto an era of abundance, I mean, they all sound like\\nDemis is being a bit overly optimistic, doesn\\'t it? But then again, he hasn\\'t\\nexactly been wrong so far. You\\'ve been listening to \"Google\\nDeepMind, the Podcast\" with me, Professor Hannah Fry. If you have enjoyed this\\nepisode, hey, why not subscribe? We\\'ve got plenty more\\nfascinating conversations with the people at\\nthe cutting edge of AI coming up on topics ranging\\nfrom how AI is accelerating the pace of\\nscientific discoveries to addressing some\\nof the biggest risks of this technology. If you have any feedback, or you\\nwant to suggest a future guest, then do leave us a\\ncomment on YouTube. Until next time. [MUSIC PLAYING]')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 - Retrieval**"
      ],
      "metadata": {
        "id": "mJgA55biCdb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "olLAbzmTCdHb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD6wRcqCCMHe",
        "outputId": "94eb7096-493b-4937-b9e5-69f8053e27e1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e993c799a00>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('What is deepmind')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-AvgsDACowG",
        "outputId": "0dd01b06-420a-4844-8490-b2c794649de6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='43ecd37e-912f-4590-b2eb-8b623fe05a8d', metadata={}, page_content='[MUSIC PLAYING] HANNAH FRY: Welcome to \"Google\\nDeepMind, the Podcast\" with me, your host, Professor Hannah Fry. Now, when we first\\nstarted thinking about making this\\npodcast way back in 2017, DeepMind was this relatively\\nsmall, focused AI research lab. They\\'d just been\\nbought by Google and given the freedom to do\\ntheir own quirky research projects from the safe\\ndistance of London. How things have changed. Because since the\\nlast season, Google has reconfigured its\\nentire structure, putting AI and the\\nteam at DeepMind at the core of its strategy. Google DeepMind has\\ncontinued its quest to endow AI with\\nhuman-level intelligence, known as artificial general\\nintelligence, or AGI. It has introduced a family of\\npowerful new AI models called Gemini, as well as\\nan AI agent called Project Astra that can process\\naudio, video, image, and code. The lab is also\\nmaking huge leaps in applying AI to a host\\nof scientific domains, including a brand new'),\n",
              " Document(id='1206fb8d-a606-4f90-bf8a-e38e6d474e9e', metadata={}, page_content=\"an AI agent called Project Astra that can process\\naudio, video, image, and code. The lab is also\\nmaking huge leaps in applying AI to a host\\nof scientific domains, including a brand new\\nthird version of AlphaFold, which can predict the structures\\nof all of the molecules that you will find in the\\nhuman body, not just proteins. And in 2021, they spun off a\\nnew company, Isomorphic Labs, to get down to the\\nbusiness of discovering new drugs to treat diseases. Google DeepMind is also working\\non powerful AI agents that can learn to perform tasks by\\nthemselves using reinforcement learning, and continuing\\nthat legacy of AlphaGo's famous victory over a\\nhuman in the game of Go. Now, of course, you'll all have\\nbeen following this podcast since the beginning. You'll all be familiar\\nwith the stories behind all of those changes. But just in case you are\\ncoming to us fresh, welcome. You can find our first\\naward-winning previous seasons on Google DeepMind's\\nYouTube channel, or wherever you\"),\n",
              " Document(id='d8055134-be58-4a0b-ad4b-97860834cbbe', metadata={}, page_content=\"thing, interestingly, that we're thinking about\\nis actually putting out a kind of principles document\\nor something before you release something to show what is the\\nexpectation from this system. What's it designed for? What's it useful for? What can't it do? And I think there is some sort\\nof education there needed of, you'll be able to find it useful\\nif you do these things with it, but don't try and use it\\nfor these other things because it won't work. And I think that\\nthat's something that we need to get better\\nat clarifying as a field, and then probably users need\\nto get more experienced on. And actually, this interesting. This is probably why chatbots\\nthemselves came a little bit out of the blue. Even obviously ChatGPT, but even\\nto OpenAI, it surprised them. And we had our own chat\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it's like, well, it's\"),\n",
              " Document(id='7413ec88-05cf-4348-8535-533861f5129b', metadata={}, page_content=\"give it an objective, they can't really do actions\\nin the world for you. So they're very much\\nlike passive Q&A systems. You put the energy in\\nby asking the question, and then they give you\\nsome kind of response. But they're not able to\\nsolve a problem for you. You can't say something\\nlike, if you wanted it as a digital assistant, you\\nmight want to say something like, book me that holiday in\\nItaly, and all the restaurants, and the museums, and whatever,\\nand it knows what you like, but then it goes out\\nand books the flights and all of that for you. So it can't do any of that. But I think that's\\nthe next era-- these more agent-based\\nsystems, we would call them,\\nor agentic systems that have agent-like behavior. But of course, that's\\nwhat we're expert in. That's what we used to build\\nwith all our game agents-- AlphaGo and all of\\nthe other things we've talked in\\nabout in the past. So a lot of what we're\\ndoing is marrying that work that we're,\\nI guess, famous for with the new large\")]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - Augmentation"
      ],
      "metadata": {
        "id": "H9gkeuUQCyiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)"
      ],
      "metadata": {
        "id": "munSR0y-Crow"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "YE57PQ9QDJn0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question= \"is the topic of nuclear fusion discussed in this video? if yes then what was discussed\"\n",
        "retrieved_docs= retriever.invoke(question)"
      ],
      "metadata": {
        "id": "hZOMFZIrDMYX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrogiYSWDa5D",
        "outputId": "f3b77da1-b528-4091-81c1-1c4177d21d8a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='1c12b0f0-db37-4609-8248-ee7c3d3625d3', metadata={}, page_content=\"cases to worry about. There's bad uses by bad\\nindividuals or nations, so human misuse, and then\\nthere's the AI itself as it gets closer to\\nAGI going off the rails. And I think you need different\\nsolutions for those two problems. And so, yeah, that's\\nwhat we're going to have to contend\\nwith as we get closer to building these technologies. And also, just going back to\\nyour benefiting everyone point, of course, we're showing\\nthe way with things like AlphaFold and isomorphic. I think we could cure most\\ndiseases within the next decade or two if AI drug design works. And then they could be\\npersonalized medicines where it minimizes the side\\neffects on the individual because it's mapped\\nto the person's individual illness, and\\ntheir individual metabolism, and so on. So these are amazing things-- clean energy, renewable\\nenergy sources, fusion, or better solar power,\\nall of these types of things. I think they're\\nall within reach. And then that would\\nsort out water access because you could do\"),\n",
              " Document(id='d8055134-be58-4a0b-ad4b-97860834cbbe', metadata={}, page_content=\"thing, interestingly, that we're thinking about\\nis actually putting out a kind of principles document\\nor something before you release something to show what is the\\nexpectation from this system. What's it designed for? What's it useful for? What can't it do? And I think there is some sort\\nof education there needed of, you'll be able to find it useful\\nif you do these things with it, but don't try and use it\\nfor these other things because it won't work. And I think that\\nthat's something that we need to get better\\nat clarifying as a field, and then probably users need\\nto get more experienced on. And actually, this interesting. This is probably why chatbots\\nthemselves came a little bit out of the blue. Even obviously ChatGPT, but even\\nto OpenAI, it surprised them. And we had our own chat\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it's like, well, it's\"),\n",
              " Document(id='c92c9acf-b3b5-425d-b3ac-ed22e879645b', metadata={}, page_content=\"things, but they have this stochastic nature,\\nprobabilistic nature. So in fact, a lot\\nof cases where if it was a normal piece of\\nsoftware, you could say I've tested 99.999% of things,\\nso then extrapolates. So then it's enough\\nbecause there's no way of exposing the flaw\\nthat it has if it has one. But that's not the case with\\nthese generative systems. They can do all\\nsorts of things that are a little bit left\\nfield, or out of the box, out of distribution, in a way,\\nfrom what you've seen before if someone clever or adversarial\\ndecides to-- it's almost like a hacker decides to\\ntest push it in some way. And it could even be-- I mean, it's so\\ncombinatorial, it could even be with all the things\\nthat you've happened to have said before to it. And then it's in some\\nkind of peculiar state which then-- or it's got\\nits memories filled up with this particular\\nthing, and then that's why it outputs something. So there's a lot of complexity\"),\n",
              " Document(id='669878a9-b313-46c1-8184-c0fb45617995', metadata={}, page_content=\"and build better world models. So actually still going\\nback to our grounding question earlier, still\\nbuilding grounding in, but piggybacking on top\\nof language this time. And so that's important. And we also had this\\nvision in the end of having a universal\\nassistant, and we prototyped something\\ncalled Astro, which I'm sure we'll\\ntalk about, which understands not just what you're\\ntyping, but actually the context you're in. And if you think about something\\nlike a personal assistant or digital assistant, it will\\nbe much more useful the more context it understood about\\nwhat you're asking it for or the situation that you're in. So we always thought that would\\nbe a much more useful type of system, and so we\\nbuilt multi-modality in from the start. So that was one thing,\\nnatively multi-modal. And then at the time, that\\nwas the only model doing that. So now the other models\\nare trying to catch up. And then the other\")]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "context_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "mEEDsVX1Ddmm",
        "outputId": "a9c7278c-c22c-4fff-9cc5-e1b2b7b097cb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"cases to worry about. There's bad uses by bad\\nindividuals or nations, so human misuse, and then\\nthere's the AI itself as it gets closer to\\nAGI going off the rails. And I think you need different\\nsolutions for those two problems. And so, yeah, that's\\nwhat we're going to have to contend\\nwith as we get closer to building these technologies. And also, just going back to\\nyour benefiting everyone point, of course, we're showing\\nthe way with things like AlphaFold and isomorphic. I think we could cure most\\ndiseases within the next decade or two if AI drug design works. And then they could be\\npersonalized medicines where it minimizes the side\\neffects on the individual because it's mapped\\nto the person's individual illness, and\\ntheir individual metabolism, and so on. So these are amazing things-- clean energy, renewable\\nenergy sources, fusion, or better solar power,\\nall of these types of things. I think they're\\nall within reach. And then that would\\nsort out water access because you could do\\n\\nthing, interestingly, that we're thinking about\\nis actually putting out a kind of principles document\\nor something before you release something to show what is the\\nexpectation from this system. What's it designed for? What's it useful for? What can't it do? And I think there is some sort\\nof education there needed of, you'll be able to find it useful\\nif you do these things with it, but don't try and use it\\nfor these other things because it won't work. And I think that\\nthat's something that we need to get better\\nat clarifying as a field, and then probably users need\\nto get more experienced on. And actually, this interesting. This is probably why chatbots\\nthemselves came a little bit out of the blue. Even obviously ChatGPT, but even\\nto OpenAI, it surprised them. And we had our own chat\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it's like, well, it's\\n\\nthings, but they have this stochastic nature,\\nprobabilistic nature. So in fact, a lot\\nof cases where if it was a normal piece of\\nsoftware, you could say I've tested 99.999% of things,\\nso then extrapolates. So then it's enough\\nbecause there's no way of exposing the flaw\\nthat it has if it has one. But that's not the case with\\nthese generative systems. They can do all\\nsorts of things that are a little bit left\\nfield, or out of the box, out of distribution, in a way,\\nfrom what you've seen before if someone clever or adversarial\\ndecides to-- it's almost like a hacker decides to\\ntest push it in some way. And it could even be-- I mean, it's so\\ncombinatorial, it could even be with all the things\\nthat you've happened to have said before to it. And then it's in some\\nkind of peculiar state which then-- or it's got\\nits memories filled up with this particular\\nthing, and then that's why it outputs something. So there's a lot of complexity\\n\\nand build better world models. So actually still going\\nback to our grounding question earlier, still\\nbuilding grounding in, but piggybacking on top\\nof language this time. And so that's important. And we also had this\\nvision in the end of having a universal\\nassistant, and we prototyped something\\ncalled Astro, which I'm sure we'll\\ntalk about, which understands not just what you're\\ntyping, but actually the context you're in. And if you think about something\\nlike a personal assistant or digital assistant, it will\\nbe much more useful the more context it understood about\\nwhat you're asking it for or the situation that you're in. So we always thought that would\\nbe a much more useful type of system, and so we\\nbuilt multi-modality in from the start. So that was one thing,\\nnatively multi-modal. And then at the time, that\\nwas the only model doing that. So now the other models\\nare trying to catch up. And then the other\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ],
      "metadata": {
        "id": "pQSNxXRxDmR0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzsdA7zIDuvG",
        "outputId": "45e7dd32-503b-4cdb-cc06-bef9eea4fdbb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      cases to worry about. There's bad uses by bad\\nindividuals or nations, so human misuse, and then\\nthere's the AI itself as it gets closer to\\nAGI going off the rails. And I think you need different\\nsolutions for those two problems. And so, yeah, that's\\nwhat we're going to have to contend\\nwith as we get closer to building these technologies. And also, just going back to\\nyour benefiting everyone point, of course, we're showing\\nthe way with things like AlphaFold and isomorphic. I think we could cure most\\ndiseases within the next decade or two if AI drug design works. And then they could be\\npersonalized medicines where it minimizes the side\\neffects on the individual because it's mapped\\nto the person's individual illness, and\\ntheir individual metabolism, and so on. So these are amazing things-- clean energy, renewable\\nenergy sources, fusion, or better solar power,\\nall of these types of things. I think they're\\nall within reach. And then that would\\nsort out water access because you could do\\n\\nthing, interestingly, that we're thinking about\\nis actually putting out a kind of principles document\\nor something before you release something to show what is the\\nexpectation from this system. What's it designed for? What's it useful for? What can't it do? And I think there is some sort\\nof education there needed of, you'll be able to find it useful\\nif you do these things with it, but don't try and use it\\nfor these other things because it won't work. And I think that\\nthat's something that we need to get better\\nat clarifying as a field, and then probably users need\\nto get more experienced on. And actually, this interesting. This is probably why chatbots\\nthemselves came a little bit out of the blue. Even obviously ChatGPT, but even\\nto OpenAI, it surprised them. And we had our own chat\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it's like, well, it's\\n\\nthings, but they have this stochastic nature,\\nprobabilistic nature. So in fact, a lot\\nof cases where if it was a normal piece of\\nsoftware, you could say I've tested 99.999% of things,\\nso then extrapolates. So then it's enough\\nbecause there's no way of exposing the flaw\\nthat it has if it has one. But that's not the case with\\nthese generative systems. They can do all\\nsorts of things that are a little bit left\\nfield, or out of the box, out of distribution, in a way,\\nfrom what you've seen before if someone clever or adversarial\\ndecides to-- it's almost like a hacker decides to\\ntest push it in some way. And it could even be-- I mean, it's so\\ncombinatorial, it could even be with all the things\\nthat you've happened to have said before to it. And then it's in some\\nkind of peculiar state which then-- or it's got\\nits memories filled up with this particular\\nthing, and then that's why it outputs something. So there's a lot of complexity\\n\\nand build better world models. So actually still going\\nback to our grounding question earlier, still\\nbuilding grounding in, but piggybacking on top\\nof language this time. And so that's important. And we also had this\\nvision in the end of having a universal\\nassistant, and we prototyped something\\ncalled Astro, which I'm sure we'll\\ntalk about, which understands not just what you're\\ntyping, but actually the context you're in. And if you think about something\\nlike a personal assistant or digital assistant, it will\\nbe much more useful the more context it understood about\\nwhat you're asking it for or the situation that you're in. So we always thought that would\\nbe a much more useful type of system, and so we\\nbuilt multi-modality in from the start. So that was one thing,\\nnatively multi-modal. And then at the time, that\\nwas the only model doing that. So now the other models\\nare trying to catch up. And then the other\\n      Question: is the topic of nuclear fusion discussed in this video? if yes then what was discussed\\n    \")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - Generation"
      ],
      "metadata": {
        "id": "dM47ZQ3PEBlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4mpY678D8o5",
        "outputId": "94a32581-3d4e-4f5c-b21d-def6f224dbac"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, nuclear fusion is mentioned as one of the technologies that could be within reach with the help of AI, potentially sorting out water access through desalination.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a Chain"
      ],
      "metadata": {
        "id": "lwk40f0GEMuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "63Q7KfHHEPJ6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "49WLQUDPEGZZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "x38-gGRxEUPP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.invoke('who is Demis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHALIol3EZOj",
        "outputId": "2cb88954-bbe1-4b0e-f3d4-23f01b6dfa1a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'are going to be harder? And then as for\\nthe big predictions that Demis made, like cures for\\nmost diseases in 10 or 20 years, or AGI by the end of\\nthe decade, or how we\\'re about to enter\\ninto an era of abundance, I mean, they all sound like\\nDemis is being a bit overly optimistic, doesn\\'t it? But then again, he hasn\\'t\\nexactly been wrong so far. You\\'ve been listening to \"Google\\nDeepMind, the Podcast\" with me, Professor Hannah Fry. If you have enjoyed this\\nepisode, hey, why not subscribe? We\\'ve got plenty more\\nfascinating conversations with the people at\\nthe cutting edge of AI coming up on topics ranging\\nfrom how AI is accelerating the pace of\\nscientific discoveries to addressing some\\nof the biggest risks of this technology. If you have any feedback, or you\\nwant to suggest a future guest, then do leave us a\\ncomment on YouTube. Until next time. [MUSIC PLAYING]\\n\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it\\'s like, well, it\\'s\\ngetting these things wrong, and it sometimes hallucinates,\\nand blah, blah, blah. And there\\'s so many things. But then what we didn\\'t\\nrealize is, actually, there\\'s still a lot of very good\\nuse cases for that even now that people find very valuable--\\nsummarizing documents, and really long\\nthings, or writing-- HANNAH FRY: Awkward emails? DEMIS HASSABIS: --awkward\\nemails, or mundane forms to be filled in. And there\\'s all these use\\ncases which, actually, people don\\'t mind if\\nthere\\'s some small errors. They can fix them easily, and\\nsaves a huge amount of time. And I guess that was\\nthe surprising thing. They discovered-- people\\ndiscovered when you put it in the hands of everyone, there\\nwere actually these valuable use cases, even though the systems\\nwere flawed in all of these ways we know. HANNAH FRY: Well, OK, so I\\n\\non track, yeah, crazily. Because usually 20-year\\nprojects stay 20 years away. But yeah, we\\'re a good way\\nin now, and I think we\\'re-- HANNAH FRY: 20 years\\nis 2030 for AGI. DEMIS HASSABIS: 2030, yeah. So I think the way I say\\nis I wouldn\\'t be surprised if it comes in the next decade. So I think we\\'re on track. HANNAH FRY: That matches\\nwhat you said last time. You haven\\'t updated your priors. [LAUGHTER] DEMIS HASSABIS: Exactly HANNAH FRY: Amazing. Demis, thank you so much. Absolute delight. Absolute delight, as always. DEMIS HASSABIS: Very fun to\\ntalk, as always, as well. Thank you. HANNAH FRY: OK, I think there\\nare a few really important things that came out of that\\nconversation, especially when you compare it to what\\nDemis was saying last time we spoke to him in 2022. Because there have definitely\\nbeen a few surprises in the last couple of years. The way that these\\nmodels have demonstrated a genuine conceptual\\nunderstanding is one-- this real world grounding\\n\\nwhen it was less people, and maybe a little bit more\\nfocused on the science. But it\\'s also good because it\\nshows that the technology is ready to impact the real\\nworld in many different ways, and impact people\\'s everyday\\nlives in positive ways. So I think it\\'s exciting, too. HANNAH FRY: Have\\nyou been surprised by how quickly this has caught\\nthe public\\'s imagination? I mean, I guess you would have\\nexpected that eventually people would have got on board. DEMIS HASSABIS: Yes, exactly. So at some point,\\nthose of us who\\'ve been working on it like us for\\nmany years now, even decades, so I guess at some\\npoint the general public would wake up to that fact. And effectively,\\neveryone\\'s starting to realize how important\\nAI is going to be. But it\\'s been\\nquite surreal still to see that actually come\\nto fruition, and for that to happen. And I guess it is the advent\\nof the chat bots and language models because everyone,\\nof course, uses language. Everyone can\\nunderstand language. So it\\'s an easy way',\n",
              " 'question': 'who is Demis'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "aL67edrnEeuM"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ],
      "metadata": {
        "id": "_JQXYfeuEmXP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke('Can you summarize the video')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "betfG1oKEoOS",
        "outputId": "3a0a04a9-fbc0-4ed5-82bb-85b36316921d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The video discusses the need for clarifying the expectations and limitations of AI systems, as well as the importance of user education. It also touches on the surprising emergence of chatbots and their inherent flaws due to their stochastic and probabilistic nature. The video also talks about the importance of technical due diligence, understanding the background of people in AI, and the opportunistic environment created by sudden attention and money in the field. It also mentions building better world models and the vision of a universal assistant with multi-modality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyQ_32iWEqKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYtttTKRKrT7HE/j24iYvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamsinghtomar78/LangChain/blob/main/Youtube_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LBIJgdYVAIP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] =\" \""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries**"
      ],
      "metadata": {
        "id": "pTBlCgIM76ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-openai \\\n",
        "               faiss-cpu tiktoken python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBWQpNgv8Ao6",
        "outputId": "6f6b6f01-64b9-4e05-ad4f-4fd28ccc93b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "k93N1DPV8PVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1a - Indexing (Document Ingestion)**"
      ],
      "metadata": {
        "id": "S-lti4xf8TZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "video_id = \"pZybROKrj2Q\"\n",
        "try:\n",
        "    api = YouTubeTranscriptApi()\n",
        "    transcript_list = api.list(video_id)\n",
        "    transcript_snippet = transcript_list.find_transcript(['en'])\n",
        "    transcript_data = transcript_snippet.fetch()\n",
        "\n",
        "    # Extract text from the FetchedTranscript object\n",
        "    transcript = \" \".join(snippet.text for snippet in transcript_data.snippets)\n",
        "\n",
        "\n",
        "except TranscriptsDisabled:\n",
        "    print(\"No captions available for this video.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "Z06srYWf-wbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1b - Indexing (Text Splitting)**"
      ],
      "metadata": {
        "id": "Cob4EQ56AmOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ],
      "metadata": {
        "id": "tURbnpC-Alxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SpiX8zzAPU9",
        "outputId": "c3c23331-8fbe-4a8b-f231-d5ffe66e09d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzy9ywcoA99A",
        "outputId": "83a841a2-6077-4c60-e652-6f60d7ac35fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"the grounding gets in by people interacting with the\\nsystem and saying that's a rubbish answer,\\nthat's a good answer. DEMIS HASSABIS: Yes. So for sure, part\\nof that, if the question that they're\\ngetting wrong, the early versions of this,\\nwas due to grounding missing-- actually, the real world\\ndogs bark in this way or whatever it is-- and it's\\nanswering it incorrectly, then that feedback\\nwill correct it. And part of that feedback is\\nfrom our own grounded knowledge. So some grounding is seeping\\nin like that for sure. HANNAH FRY: I remember\\nseeing a really nice example about crossing the English\\nChannel versus walking across the English Channel. DEMIS HASSABIS: Exactly,\\nthose kinds of things. And if it answered wrong,\\nyou would tell it it's wrong. And then it would have\\nto slightly figure out that you can't walk\\nacross the Channel. HANNAH FRY: So some\\nof these properties that have emerged that\\nweren't necessarily expected to be, I want to ask\")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)**"
      ],
      "metadata": {
        "id": "rBft_Ey9BFgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "yW0CKqf9BBAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 - Retrieval**"
      ],
      "metadata": {
        "id": "mJgA55biCdb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "olLAbzmTCdHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD6wRcqCCMHe",
        "outputId": "94eb7096-493b-4937-b9e5-69f8053e27e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e993c799a00>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - Augmentation"
      ],
      "metadata": {
        "id": "H9gkeuUQCyiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)"
      ],
      "metadata": {
        "id": "munSR0y-Crow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "YE57PQ9QDJn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question= \"is the topic of nuclear fusion discussed in this video? if yes then what was discussed\"\n",
        "retrieved_docs= retriever.invoke(question)"
      ],
      "metadata": {
        "id": "hZOMFZIrDMYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "context_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "mEEDsVX1Ddmm",
        "outputId": "a9c7278c-c22c-4fff-9cc5-e1b2b7b097cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"cases to worry about. There's bad uses by bad\\nindividuals or nations, so human misuse, and then\\nthere's the AI itself as it gets closer to\\nAGI going off the rails. And I think you need different\\nsolutions for those two problems. And so, yeah, that's\\nwhat we're going to have to contend\\nwith as we get closer to building these technologies. And also, just going back to\\nyour benefiting everyone point, of course, we're showing\\nthe way with things like AlphaFold and isomorphic. I think we could cure most\\ndiseases within the next decade or two if AI drug design works. And then they could be\\npersonalized medicines where it minimizes the side\\neffects on the individual because it's mapped\\nto the person's individual illness, and\\ntheir individual metabolism, and so on. So these are amazing things-- clean energy, renewable\\nenergy sources, fusion, or better solar power,\\nall of these types of things. I think they're\\nall within reach. And then that would\\nsort out water access because you could do\\n\\nthing, interestingly, that we're thinking about\\nis actually putting out a kind of principles document\\nor something before you release something to show what is the\\nexpectation from this system. What's it designed for? What's it useful for? What can't it do? And I think there is some sort\\nof education there needed of, you'll be able to find it useful\\nif you do these things with it, but don't try and use it\\nfor these other things because it won't work. And I think that\\nthat's something that we need to get better\\nat clarifying as a field, and then probably users need\\nto get more experienced on. And actually, this interesting. This is probably why chatbots\\nthemselves came a little bit out of the blue. Even obviously ChatGPT, but even\\nto OpenAI, it surprised them. And we had our own chat\\nbots, and Google had theirs. And one of the things was\\nwe were looking at them, and we were looking at all\\nthe flaws they still had, and they still do. And it's like, well, it's\\n\\nthings, but they have this stochastic nature,\\nprobabilistic nature. So in fact, a lot\\nof cases where if it was a normal piece of\\nsoftware, you could say I've tested 99.999% of things,\\nso then extrapolates. So then it's enough\\nbecause there's no way of exposing the flaw\\nthat it has if it has one. But that's not the case with\\nthese generative systems. They can do all\\nsorts of things that are a little bit left\\nfield, or out of the box, out of distribution, in a way,\\nfrom what you've seen before if someone clever or adversarial\\ndecides to-- it's almost like a hacker decides to\\ntest push it in some way. And it could even be-- I mean, it's so\\ncombinatorial, it could even be with all the things\\nthat you've happened to have said before to it. And then it's in some\\nkind of peculiar state which then-- or it's got\\nits memories filled up with this particular\\nthing, and then that's why it outputs something. So there's a lot of complexity\\n\\nand build better world models. So actually still going\\nback to our grounding question earlier, still\\nbuilding grounding in, but piggybacking on top\\nof language this time. And so that's important. And we also had this\\nvision in the end of having a universal\\nassistant, and we prototyped something\\ncalled Astro, which I'm sure we'll\\ntalk about, which understands not just what you're\\ntyping, but actually the context you're in. And if you think about something\\nlike a personal assistant or digital assistant, it will\\nbe much more useful the more context it understood about\\nwhat you're asking it for or the situation that you're in. So we always thought that would\\nbe a much more useful type of system, and so we\\nbuilt multi-modality in from the start. So that was one thing,\\nnatively multi-modal. And then at the time, that\\nwas the only model doing that. So now the other models\\nare trying to catch up. And then the other\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ],
      "metadata": {
        "id": "pQSNxXRxDmR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - Generation"
      ],
      "metadata": {
        "id": "dM47ZQ3PEBlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4mpY678D8o5",
        "outputId": "94a32581-3d4e-4f5c-b21d-def6f224dbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, nuclear fusion is mentioned as one of the technologies that could be within reach with the help of AI, potentially sorting out water access through desalination.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a Chain"
      ],
      "metadata": {
        "id": "lwk40f0GEMuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "63Q7KfHHEPJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "49WLQUDPEGZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "x38-gGRxEUPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "aL67edrnEeuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ],
      "metadata": {
        "id": "_JQXYfeuEmXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke('Can you summarize the video')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "betfG1oKEoOS",
        "outputId": "3a0a04a9-fbc0-4ed5-82bb-85b36316921d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The video discusses the need for clarifying the expectations and limitations of AI systems, as well as the importance of user education. It also touches on the surprising emergence of chatbots and their inherent flaws due to their stochastic and probabilistic nature. The video also talks about the importance of technical due diligence, understanding the background of people in AI, and the opportunistic environment created by sudden attention and money in the field. It also mentions building better world models and the vision of a universal assistant with multi-modality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyQ_32iWEqKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}